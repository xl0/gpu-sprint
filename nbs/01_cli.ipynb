{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |hide\n",
    "# |export\n",
    "import platform\n",
    "\n",
    "import torch\n",
    "import timm\n",
    "import wandb\n",
    "\n",
    "from fastcore.script import *\n",
    "\n",
    "from gpu_sprint.benchmark import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |export\n",
    "@call_parse\n",
    "def cli(wnb: str =\"disabled\",       # W&B mode. Accepted values: online, offline, disabled.\n",
    "        wnb_run: str =None,    # W&B run name (auto-generate if None)\n",
    "        wnb_project: str =None,\n",
    "        wnb_entity: str =None,\n",
    "        run_number = 1,             # A unique number to keep track over repeat runs\n",
    "        model: str =\"resnet50\",     # TIMM Model name\n",
    "        bs: int =32,                #Batch size\n",
    "        size: int =224,             # (fake) image size\n",
    "        fp16: bool =False,\n",
    "        n_batches = 0,              # Run for N batches. Mututally exclusive with `n_seconds`\n",
    "        n_seconds = 0,              # Run for N seconds. Mutually exclusive with `n_batches`\n",
    "    ):\n",
    "\n",
    "    if not n_batches and not n_seconds:\n",
    "        print(\"Either `n_batches` or `n_seconds` must be non-zero\")\n",
    "        exit(1)\n",
    "    if n_batches and n_seconds:\n",
    "        print(f\"`n_batches` and `n_seconds` are mutually explusive. {n_batches=}, {n_seconds=}\")\n",
    "        exit(1)\n",
    "\n",
    "    assert wnb in [\"online\", \"offline\", \"disabled\"]\n",
    "    \n",
    "    stats = {\n",
    "        \"device_name\" : torch.cuda.get_device_name(),\n",
    "        \"device_capability\": str(torch.cuda.get_device_capability()),\n",
    "        \"pytorch\": torch.version.__version__,\n",
    "        \"cuda\": torch.version.cuda,\n",
    "        \"platform\": platform.platform(),\n",
    "        \"model\": model,\n",
    "        \"fp16\": fp16,\n",
    "        \"bs\": bs,\n",
    "        \"n_seconds\": n_seconds if n_seconds else None,\n",
    "        \"n_batches\": n_batches if n_batches else None,\n",
    "        \"run_number\": run_number, \n",
    "    }\n",
    "\n",
    "    print(\"\\n\".join([ f\"{k}: {v}\" for k, v in stats.items() ]))\n",
    "\n",
    "    with wandb.init(mode=wnb, project=wnb_project, entity=wnb_entity, name=wnb_run) as run:\n",
    "        run.log(stats, step=0, commit=True)\n",
    "\n",
    "        model = timm.create_model(model, pretrained=False)\n",
    "        duration, n_items = benchmark(model, bs=bs, size=size, fp16=fp16, n_batches=n_batches, n_seconds=n_seconds)\n",
    "        summary = {\n",
    "            \"duration\": duration,\n",
    "            \"n_items\": n_items,\n",
    "            \"throughput\": n_items / duration,\n",
    "        }\n",
    "\n",
    "        print(\"\\n\".join([ f\"{k}: {v}\" for k, v in summary.items() ]))\n",
    "\n",
    "        run.log(summary, step=0, commit=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# |eval: false\n",
    "# |hide\n",
    "cli(wnb = \"disabled\",\n",
    "    wnb_run=\"hbfs\",\n",
    "    wnb_project=\"hbfs\",\n",
    "    model=\"resnet50\",\n",
    "    bs=32,\n",
    "    size=224,\n",
    "    fp16=False,\n",
    "    n_batches = 0,\n",
    "    n_seconds = 20,\n",
    "    run_number=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: gpu-sprint [-h] [--wnb WNB] [--wnb_run WNB_RUN]\n",
      "                  [--wnb_project WNB_PROJECT] [--wnb_entity WNB_ENTITY]\n",
      "                  [--model MODEL] [--bs BS] [--size SIZE] [--fp16]\n",
      "                  [--n_batches N_BATCHES] [--n_seconds N_SECONDS]\n",
      "\n",
      "options:\n",
      "  -h, --help                 show this help message and exit\n",
      "  --wnb WNB                  W&B mode. Accepted values: online, offline,\n",
      "                             disabled. (default: disabled)\n",
      "  --wnb_run WNB_RUN          W&B run name (auto-generate if None)\n",
      "  --wnb_project WNB_PROJECT\n",
      "  --wnb_entity WNB_ENTITY\n",
      "  --model MODEL              TIMM Model name (default: resnet50)\n",
      "  --bs BS                    Batch size (default: 32)\n",
      "  --size SIZE                (fake) image size (default: 224)\n",
      "  --fp16                     (default: False)\n",
      "  --n_batches N_BATCHES      Run for N batches. Mututally exclusive with\n",
      "                             `n_seconds` (default: 0)\n",
      "  --n_seconds N_SECONDS      Run for N seconds. Mutually exclusive with\n",
      "                             `n_batches` (default: 0)\n"
     ]
    }
   ],
   "source": [
    "!gpu-sprint -h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the benchmark over a range of models and batch sizes, have a look at the `run_all_benahmarks.sh` script:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#!/bin/bash\n",
      "\n",
      "WANDB_MODE=\"online\"\n",
      "WANDB_PROJECT=\"gpu-sprint2\"\n",
      "\n",
      "\n",
      "MODELS=\"resnet50 swin_s3_tiny_224\"\n",
      "BATCHES=\"8 16 32 64 128 256 512 1024\"\n",
      "\n",
      "set -x\n",
      "\n",
      "echo \"Warming up the GPU for 3 minutes...\"\n",
      "gpu-sprint --model=resnet50 --n_seconds=180\n",
      "\n",
      "echo \"Running benchmarks...\"\n",
      "for m in $MODELS\n",
      "do\n",
      "    for fp16 in \" \" \"--fp16\"\n",
      "    do\n",
      "        for bs in $BATCHES\n",
      "        do\n",
      "            gpu-sprint --model=$m $fp16 --bs=$bs --n_seconds=30 --wnb=$WANDB_MODE --wnb_project=$WANDB_PROJECT\n",
      "            if [ $? -ne 0 ]\n",
      "            then\n",
      "                break # We probably hit a batch size the GPU can't handle\n",
      "            fi\n",
      "\n",
      "        done\n",
      "    done\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "#!hide\n",
    "!cat ../run_all_benchmarks.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 64-bit",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
