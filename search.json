[
  {
    "objectID": "cli.html",
    "href": "cli.html",
    "title": "CLI",
    "section": "",
    "text": "pip install ready-steady-go"
  },
  {
    "objectID": "cli.html#how-to-use",
    "href": "cli.html#how-to-use",
    "title": "CLI",
    "section": "How to use",
    "text": "How to use\n\n\nusage: ready-steady-go [-h] [--wnb WNB] [--wnb_run WNB_RUN]\n                       [--wnb_project WNB_PROJECT] [--wnb_entity WNB_ENTITY]\n                       [--run_number RUN_NUMBER] [--model MODEL] [--bs BS]\n                       [--size SIZE] [--fp16] [--n_batches N_BATCHES]\n                       [--n_seconds N_SECONDS]\n\noptions:\n  -h, --help                 show this help message and exit\n  --wnb WNB                  W&B mode. Accepted values: online, offline,\n                             disabled. (default: disabled)\n  --wnb_run WNB_RUN          W&B run name (auto-generate if None)\n  --wnb_project WNB_PROJECT\n  --wnb_entity WNB_ENTITY\n  --run_number RUN_NUMBER    A unique number to keep track over repeat runs\n                             (default: 1)\n  --model MODEL              TIMM Model name (default: resnet50)\n  --bs BS                    Batch size (default: 32)\n  --size SIZE                (fake) image size (default: 224)\n  --fp16                     (default: False)\n  --n_batches N_BATCHES      Run for N batches. Mututally exclusive with\n                             `n_seconds` (default: 0)\n  --n_seconds N_SECONDS      Run for N seconds. Mutually exclusive with\n                             `n_batches` (default: 0)\n\n\n\nBatch mode\nTo run the benchmark over a range of models and batch sizes, have a look at the run_all_benahmarks.sh script:\n\n\n#!/bin/bash\n\nWANDB_MODE=\"online\"\nWANDB_PROJECT=\"ready-steady-go\"\n\nMODELS=\"resnet50 vgg19 swin_s3_base_224\"\nBATCHES=\"8 16 32 64 128 256 512 1024 2048 4096\"\n\nN_SECONDS=30\n\n#set -x\n\nnvidia-smi -q > gpu-info.txt\ncat /proc/cpuinfo > cpu-info.txt\n\nwandb login\n\necho \"Warming up the GPU for 3 minutes...\"\nready-steady-go --model=resnet50 --n_seconds=180\n\necho \"Running benchmarks...\"\n\n# You can do multiple runs, but in my experience the results barely change between runs.\nfor RUN in 1 #2 3\ndo\n    for m in $MODELS; do\n        for fp16 in \" \" \"--fp16\"; do\n            for bs in $BATCHES; do\n                ready-steady-go --model=$m $fp16 --bs=$bs --n_seconds=$N_SECONDS \\\n                    --wnb=$WANDB_MODE --wnb_project=$WANDB_PROJECT --run_number=$RUN\n                if [ $? -ne 0 ]; then\n                    # We probably hit a batch size the GPU can't handle.\n                    # No need to try larger batch sizes.\n                    break\n                fi\n            done\n        done\n    done\ndone\n\n# Sync everything just in case. On a rare occasion wandb forgets to update symmary otherwise.\nwandb sync --sync-all --include-synced"
  },
  {
    "objectID": "benchmark.html",
    "href": "benchmark.html",
    "title": "Benchmark",
    "section": "",
    "text": "Run a standard PyTorch training loop on an image classifier model of your choice with specified batch size and FP16/FP32. The result is the measure of throughput - number of trainig samples per second. It can be synced to Wights & Biases. See more in CLI\n\nNote: The data never leaves the GPU, and the throughput should be mostly independent of the rest of the system, at least for larger batch sizes.\n\n\ndef benchmark(model: nn.Module, # Model to run\n                bs: int =32,    # Batch size\n                n_batches: int =None,  # Number of batches to run. `seconds` must be None\n                n_seconds: int =None,  # Number of seconds to run. `n_batches` must be None\n                fp16: int =False,      # Use Automatic Mixed Precision\n                size: int=224,         # Mock-train on this size \"images\"\n                dev: torch.device=torch.device(\"cuda:0\"),): # Device to run on\n    \"\"\"Mock-train the model on random noise input.\"\"\"\n\n    # There can be only one\n    assert not n_batches or not n_seconds\n    assert n_batches or n_seconds\n\n\n    torch.backends.cudnn.benchmark=True\n    assert torch.backends.cudnn.is_available()\n\n    model.to(dev)\n    optim = torch.optim.SGD(model.parameters(), lr=0.00001, weight_decay=0.00005, momentum=0)\n\n    state = { k : v.cpu() for k,v in model.state_dict().items() }\n\n\n    X = torch.randn((bs, 3, size, size), device=dev)\n    y = torch.randint(0, 999, (bs,), device=dev)\n\n    if n_batches:\n        pbar = tqdm(total=n_batches, unit=\"Batch\")\n    else:\n        pbar = tqdm(total=n_seconds,\n            bar_format=\"{l_bar}{bar}| {n:.1f}/{total} s [{elapsed}<{remaining} {postfix}]\")\n    \n    start_time = last_time = 0\n    for c in count():\n\n        model.load_state_dict(state)\n\n        with autocast(enabled=fp16):\n            yhat = model(X)\n            loss = F.cross_entropy(yhat, y)\n\n        loss.backward()\n        optim.step()\n\n        tt=time.time()\n        optim.zero_grad(set_to_none=True)\n\n        if not start_time:\n            last_time = start_time = tt\n        else:\n            if n_batches:\n                pbar.update()\n                # Note: c starts with 0, but we discard the first iteration\n                if c == n_batches:\n                    break\n            else:\n                iter_time =  tt - last_time\n                run_time = tt - start_time\n                pbar.update(iter_time)\n                if run_time >= n_seconds:\n                    break\n                last_time = tt\n\n    pbar.close()\n\n    return ((time.time() - start_time), c*bs)\n\n\nsource\n\nbenchmark\n\n benchmark (model:torch.nn.modules.module.Module, bs:int=32,\n            n_batches:int=None, n_seconds:int=None, fp16:int=False,\n            size:int=224, dev:torch.device=device(type='cuda', index=0))\n\nMock-train the model on random noise input.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel\nModule\n\nModel to run\n\n\nbs\nint\n32\nBatch size\n\n\nn_batches\nint\nNone\nNumber of batches to run. seconds must be None\n\n\nn_seconds\nint\nNone\nNumber of seconds to run. n_batches must be None\n\n\nfp16\nint\nFalse\nUse Automatic Mixed Precision\n\n\nsize\nint\n224\nMock-train on this size “images”\n\n\ndev\ndevice\ncuda:0\nDevice to run on\n\n\n\n\nmodel = timm.create_model(\"vgg11\", pretrained=False)\nbenchmark(model, n_seconds=10)\n\n\n\n\n(10.038218975067139, 1888)\n\n\n\nbenchmark(model, n_batches=10)\n\n\n\n\n(1.6976494789123535, 320)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Ready, Steady, Go!",
    "section": "",
    "text": "It’s fall 2022, and for the first time in years, buying a GPU for Deep Learning experiments does not sound too crazy.\nNow, how do we pick one?\n\nKeep in mind, performance depends on many factors, not least your CPU and often SSD.\nFor experiments, you might be better off with 2 cheaper GPUs - one to run in background, the other used interactively.\n\n\nMy Results\n\n\nImports\nimport wandb\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport numpy as np\n\n\nI ran the benchmark on a variety of GPUs from vast.ai. The results are automatically synced to Weights & Biases.\n\n\nRetrieve and clean up data\nsns.set_theme(style=\"whitegrid\")\nsns.set_color_codes(\"pastel\")\n\napi = wandb.Api()\n\nruns = api.runs(\"xl0/ready-steady-go\")\nsummaries = [ dict(r.summary) | {\"id\": r.id} for r in runs if r.state == \"finished\"]\n\ndf = pd.DataFrame.from_records(summaries)\n\ndf = df[[\"device_name\", \"model\", \"bs\", \"fp16\", \"throughput\"]]\ndf[\"fp16\"] = df[\"fp16\"].apply(lambda x: \"FP16\" if x else \"FP32\")\ndf = df.replace({\"device_name\" : {\n                        \"NVIDIA*\": \"\",\n                        \"GeForce\": \"\",\n                        \"Tesla\": \"\",\n                        \"-\": \" \"}}, regex=True)\ndf.dropna(inplace=True)\n\n# For each model, normalize performance by top throughput.\nfor model in df.model.unique():\n    df.loc[ df.model == model, \"throughput\"] /= df.loc[df.model == model, \"throughput\"].max()\ndf[\"throughput\"] *= 100\n\n\n\n\nPlot GPU performance\n# For each device+model+fp, get the index of the entry with the highest throughput.\nmax_bs_idx = df.groupby([\"device_name\", \"model\", \"fp16\"])[\"throughput\"].idxmax()\n\nfor model in df.model.unique():   \n    f, ax = plt.subplots(figsize=(15, 6))\n    \n    tops = df.loc[max_bs_idx].query(f\"model == '{model}'\").sort_values(\"throughput\", ascending=False)\n\n    sns.set_color_codes(\"pastel\")\n    sns.barplot(ax=ax, data=tops.query(\"fp16 == 'FP16'\"),\n                x=\"throughput\", y=\"device_name\", label=\"FP16\", color=\"b\", alpha=1)\n\n    sns.set_color_codes(\"muted\")\n    sns.barplot(ax=ax, data=tops.query(\"fp16 == 'FP32'\"),\n                x=\"throughput\", y=\"device_name\", label=\"FP32\", color=\"b\", alpha=0.8,\n                order=tops.query(\"fp16 == 'FP16'\").sort_values(\"throughput\", ascending=False).device_name)\n\n    ax.legend(ncol=2, loc=\"lower right\", frameon=True)\n    ax.set(ylabel=None, xlabel=None, title=model)\n\n\n\n\n\n\n\n\n\n\n\n\n\nAverage\nf, ax = plt.subplots(figsize=(15, 6))\n\ntops = df.loc[max_bs_idx].sort_values(\"throughput\", ascending=False)\n# tops\n\n\nfp16s = df.loc[max_bs_idx].query(\"fp16=='FP16'\")\ngrouped = fp16s.groupby([\"device_name\"], as_index=False)[\"throughput\"]\n\ndisplay_order = grouped.mean().sort_values(\"throughput\", ascending=False)\n\n\nsns.set_color_codes(\"pastel\")\ntt= sns.barplot(ax=ax, data=tops.loc[tops.fp16.eq('FP16')],\n            x=\"throughput\", y=\"device_name\", label=\"FP16\", color=\"b\", errwidth=0,\n            order=display_order.device_name)\n\n# f, ax = plt.subplots(figsize=(15, 6))\n\nsns.set_color_codes(\"muted\")\nsns.barplot(ax=ax, data=tops.loc[tops.fp16.eq('FP32')],\n            x=\"throughput\", y=\"device_name\", label=\"FP32\", color=\"b\", errwidth=0,\n            order=display_order.device_name)\n        \n\nax.legend(ncol=2, loc=\"lower right\", frameon=True)\n_ = ax.set(ylabel=None, xlabel=None, title=\"Average between all models\")"
  }
]